project_name
==============================

Bike sharing dataset MLOps project

Project Organization
------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- Aqui se guardan los csv TRAIN y TEST limpios.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump AQUI ESTA MODIFIED CSV.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details
    ‚îú‚îÄ‚îÄ metrics             <- JSON FILES GENERADOS EN EVALUATE
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- MODELOS ENTRENADOS EN TRAIN
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-jqp-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py       <- Main script por correr, contiene Osquestrator y run()
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data.py       <- DataLoader y DataPreprocessor classes, se usa en stage_data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ train_predict.py       <- Model y Evaluator clases, se usa en stage_train y stage_evaluate
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ visualize.py <- Visualizer class, se usa en stage_visualize
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>

## üöÄ INICIO R√ÅPIDO (Para nuevos usuarios)

Si es tu primera vez con este proyecto, sigue estos pasos en orden:

1. **Crear ambiente virtual** (conda o venv) e instalar dependencias
2. **Configurar credenciales AWS** (crear archivo `.env` desde `.env.example`)
3. **Preparar datos** (copiar CSV a `data/raw/`)
4. **Iniciar servidor MLflow** (ejecutar `./start_mlflow.sh`)
5. **Ejecutar pipeline** (stages: DATA ‚Üí TRAIN ‚Üí EVALUATE ‚Üí VISUALIZE)

üìñ **Lee las secciones detalladas abajo si tienes dudas.**

---

## SETUP INICIAL (Primera vez)

### 1. Crear y activar ambiente virtual

**Opci√≥n A: Con Conda (recomendado)**
```bash
# Crear ambiente
conda create -n proyectomlops python=3.11 -y

# Activar ambiente
conda activate proyectomlops

# Instalar dependencias
pip install -r requirements.txt
```

**Opci√≥n B: Con venv (Python nativo)**
```bash
# Crear ambiente virtual
python3 -m venv venv

# Activar ambiente
source venv/bin/activate  # En Mac/Linux
# O en Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

**Opci√≥n C: Si ya tienes un ambiente creado**
```bash
# Solo activar tu ambiente existente
conda activate proyectomlops  # Si usas conda
# O
source venv/bin/activate      # Si usas venv

# Instalar/actualizar dependencias
pip install -r requirements.txt
```

### 2. Configurar credenciales AWS
Las credenciales est√°n en el archivo `202502-equipo4_accessKeys.csv` (compartido por el equipo). 

```bash
# Copiar el template y editarlo con tus credenciales
cp .env.example .env

# Editar .env y reemplazar YOUR_ACCESS_KEY_ID y YOUR_SECRET_ACCESS_KEY
# con las credenciales del archivo accessKeys.csv
```

**Contenido del `.env`:**
```bash
# AWS S3 Credentials (obtener del archivo accessKeys.csv)
AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION=us-east-1

# MLflow Configuration
MLFLOW_TRACKING_URI=http://127.0.0.1:5000
MLFLOW_S3_BUCKET=itesm-mna
MLFLOW_ARTIFACT_ROOT=s3://itesm-mna/202502-equipo4/mlflow-artifacts
```

‚ö†Ô∏è **IMPORTANTE:** 
- Reemplazar `YOUR_ACCESS_KEY_ID` y `YOUR_SECRET_ACCESS_KEY` con las credenciales reales
- El archivo `.env` NO debe subirse a Git (ya est√° en `.gitignore`)

### 3. Preparar datos
```bash
mkdir -p data/raw
cp data/bike_sharing_modified.csv data/raw/
```

### 4. Iniciar servidor MLflow

**IMPORTANTE:** El servidor MLflow debe estar corriendo ANTES de ejecutar cualquier stage del pipeline.

```bash
# Dar permisos de ejecuci√≥n al script (solo primera vez)
chmod +x start_mlflow.sh

# Iniciar servidor en background
nohup ./start_mlflow.sh > mlflow_server.log 2>&1 &

# Esperar 5 segundos para que inicie
sleep 5

# Verificar que est√° corriendo (debe responder: OK)
curl http://127.0.0.1:5000/
```

**Si ves "OK", el servidor est√° listo. Si no:**
```bash
# Ver el log para detectar errores
tail -20 mlflow_server.log

# Posibles problemas:
# - Puerto 5000 ocupado: lsof -ti:5000 | xargs kill -9
# - Falta archivo .env: verificar que existe y tiene las credenciales
```

## EJECUTAR PIPELINE

### Opci√≥n 1: Pipeline Completo con DVC (Recomendado)

Ejecuta todos los stages autom√°ticamente en orden con un solo comando:

```bash
# Aseg√∫rate de tener el ambiente activado y credenciales exportadas
conda activate proyectomlops  # o source venv/bin/activate

export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000

# Ejecutar pipeline completo
dvc repro
```

**¬øQu√© hace `dvc repro`?**
- Ejecuta autom√°ticamente: DATA ‚Üí TRAIN ‚Üí EVALUATE ‚Üí VISUALIZE
- Solo re-ejecuta stages que cambiaron (caching inteligente)
- Genera `dvc.lock` para reproducibilidad
- Trackea dependencias entre stages

**Salida esperada:**
```
'data/raw.dvc' didn't change, skipping
Running stage 'data'...
Running stage 'train'...
Running stage 'evaluate'...
Running stage 'visualize'...
Updating lock file 'dvc.lock'
```

---

### Opci√≥n 2: Ejecutar Stages Individualmente

Si prefieres ejecutar cada stage por separado:

### ‚ö†Ô∏è ANTES DE EJECUTAR CUALQUIER STAGE:

**1. Aseg√∫rate de tener el ambiente activado:**
```bash
conda activate proyectomlops  # Si usas conda
# O
source venv/bin/activate      # Si usas venv
```

**2. Aseg√∫rate de que el servidor MLflow est√© corriendo:**
```bash
curl http://127.0.0.1:5000/health  # Debe responder: OK
```

**3. Exportar credenciales AWS (reemplazar con tus credenciales del archivo accessKeys.csv):**
```bash
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000
```

**üí° TIP:** Puedes crear un script `set_env.sh` con estos exports para no escribirlos cada vez:
```bash
# Crear archivo set_env.sh
cat > set_env.sh << 'EOF'
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000
EOF

# Luego solo ejecutar:
source set_env.sh
```

### Ejecutar Stages en Orden

**Los stages deben ejecutarse en este orden:** DATA ‚Üí TRAIN ‚Üí EVALUATE ‚Üí VISUALIZE

#### Stage 1: DATA (Procesamiento de datos)
Procesa los datos crudos y genera conjuntos de train/test limpios.

```bash
python -m src.main \
    --stage=data \
    --csv data/raw/bike_sharing_modified.csv \
    --target cnt \
    --cleaned_train_csv data/processed/bike_sharing_train_cleaned.csv \
    --cleaned_test_csv data/processed/bike_sharing_test_cleaned.csv
```

**Salida esperada:**
- `data/processed/bike_sharing_train_cleaned.csv` (train set limpio)
- `data/processed/bike_sharing_test_cleaned.csv` (test set limpio)

---

#### Stage 2: TRAIN (Entrenamiento de modelos)
Entrena 3 modelos: Random Forest, Gradient Boosting y Ridge Regression.

```bash
python -m src.main \
    --stage=train \
    --cleaned_train_csv data/processed/bike_sharing_train_cleaned.csv \
    --target cnt \
    --models_dir models
```

**Salida esperada:**
- `models/random_forest.pkl`
- `models/gradient_boosting.pkl`
- `models/ridge_regression.pkl`
- Metadata JSON para cada modelo
- Artifacts en S3

‚è±Ô∏è **Tiempo estimado:** 2-5 minutos (GridSearchCV con 270 fits)

---

#### Stage 3: EVALUATE (Evaluaci√≥n de modelos)
Eval√∫a los modelos entrenados en el test set.

```bash
python -m src.main \
    --stage=evaluate \
    --models_dir models \
    --cleaned_test_csv data/processed/bike_sharing_test_cleaned.csv \
    --target cnt \
    --metrics_dir metrics
```

**Salida esperada:**
- `metrics/random_forest_test_results.json`
- `metrics/gradient_boosting_test_results.json`
- `metrics/ridge_regression_test_results.json`

---

#### Stage 4: VISUALIZE (Visualizaci√≥n y reportes)
Genera gr√°ficas de comparaci√≥n y reportes.

```bash
python -m src.main \
    --stage=visualize \
    --metrics_dir metrics \
    --reports_dir reports
```

**Salida esperada:**
- `reports/model_comparison.png` (gr√°fica de comparaci√≥n)
- `reports/model_comparison_results.csv` (tabla de resultados)
- `reports/performance_report.md` (reporte en Markdown)
- Artifacts en S3

## ACCEDER A MLFLOW UI
```bash
# Abrir en navegador
open http://127.0.0.1:5000
```

## DETENER SERVIDOR MLFLOW
```bash
pkill -f "mlflow server"
```

## TROUBLESHOOTING

### Problemas Comunes y Soluciones

#### ‚ùå Error: "ModuleNotFoundError: No module named 'pandas'"
**Causa:** No instalaste las dependencias o no activaste el ambiente.

**Soluci√≥n:**
```bash
# Activar ambiente
conda activate proyectomlops  # o source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

---

#### ‚ùå Error: "NoSuchBucket" o "Failed to upload to S3"
**Causa:** Credenciales AWS no configuradas o incorrectas.

**Soluci√≥n:**
```bash
# 1. Verificar que .env existe y tiene las credenciales correctas
cat .env

# 2. Exportar credenciales en la terminal
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1

# 3. Verificar acceso a S3
aws s3 ls s3://itesm-mna/202502-equipo4/
```

---

#### ‚ùå Error: "Address already in use" (puerto 5000)
**Causa:** Ya hay un proceso usando el puerto 5000.

**Soluci√≥n:**
```bash
# Matar proceso en puerto 5000
lsof -ti:5000 | xargs kill -9

# Reiniciar servidor MLflow
nohup ./start_mlflow.sh > mlflow_server.log 2>&1 &
sleep 5
curl http://127.0.0.1:5000/health
```

---

#### ‚ùå Error: "FileNotFoundError: data/raw/bike_sharing_modified.csv"
**Causa:** El archivo de datos no est√° en la ubicaci√≥n correcta.

**Soluci√≥n:**
```bash
# Crear directorio y copiar archivo
mkdir -p data/raw
cp data/bike_sharing_modified.csv data/raw/
```

---

#### ‚ùå Error: "MLFLOW_TRACKING_URI not set"
**Causa:** Variable de entorno no exportada.

**Soluci√≥n:**
```bash
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000

# O verificar que el servidor MLflow est√© corriendo
curl http://127.0.0.1:5000/health
```

---

#### ‚ùå El servidor MLflow no inicia
**Causa:** Error en el script o credenciales incorrectas.

**Soluci√≥n:**
```bash
# Ver el log de errores
tail -50 mlflow_server.log

# Verificar que .env existe
ls -la .env

# Intentar iniciar manualmente para ver errores
./start_mlflow.sh
```

---

#### üí° Verificar que todo est√° configurado correctamente

```bash
# 1. Ambiente activado
which python  # Debe mostrar ruta del ambiente virtual

# 2. Dependencias instaladas
pip list | grep -E "mlflow|pandas|scikit-learn|boto3"

# 3. Servidor MLflow corriendo
curl http://127.0.0.1:5000/health  # Debe responder: OK

# 4. Credenciales AWS configuradas
echo $AWS_ACCESS_KEY_ID  # Debe mostrar tu access key

# 5. Datos en lugar correcto
ls -lh data/raw/bike_sharing_modified.csv
```

---

### üìö M√°s Informaci√≥n

Ver `SETUP_INSTRUCTIONS.md` para una gu√≠a m√°s detallada.

Argumento	Requerido	Descripci√≥n
--stage	S√≠	Define la etapa del pipeline a ejecutar. Las opciones v√°lidas son:
data: procesa los datos y genera un CSV limpio.
train: entrena los modelos definidos en MODEL_CONFIGS.
evaluate: eval√∫a los modelos entrenados y genera m√©tricas.
visualize: genera gr√°ficas y reportes de desempe√±o.
--csv	S√≠ (solo para --stage=data)	Ruta al archivo CSV original que se procesar√°.
--processed_csv	No	Ruta del archivo CSV procesado (por defecto: data/processed/processed.csv). Usado por las etapas train y evaluate.
--models_dir	No	Directorio donde se guardan o cargan los modelos entrenados (por defecto: models/).
--metrics_dir	No	Directorio donde se guardan o leen las m√©tricas de evaluaci√≥n en formato JSON (por defecto: metrics/).
--reports_dir	No	Directorio donde se generan los gr√°ficos y reportes de desempe√±o (por defecto: reports/).
--target	No	Nombre de la variable objetivo (columna dependiente). Si no se especifica, se usa la √∫ltima columna del dataset o una llamada target.
--test_size	No	Proporci√≥n de datos destinados al conjunto de prueba. Valor por defecto: 0.2.
--random_state	No	Semilla aleatoria para asegurar reproducibilidad. Valor por defecto: 42.
