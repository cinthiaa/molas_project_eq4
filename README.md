project_name
==============================

Bike sharing dataset MLOps project

Project Organization
------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- Aqui se guardan los csv TRAIN y TEST limpios.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump AQUI ESTA MODIFIED CSV.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details
    ‚îú‚îÄ‚îÄ metrics             <- JSON FILES GENERADOS EN EVALUATE
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- MODELOS ENTRENADOS EN TRAIN
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-jqp-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py       <- Main script por correr, contiene Osquestrator y run()
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data.py       <- DataLoader y DataPreprocessor classes, se usa en stage_data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ train_predict.py       <- Model y Evaluator clases, se usa en stage_train y stage_evaluate
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ visualize.py <- Visualizer class, se usa en stage_visualize
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>

## üöÄ INICIO R√ÅPIDO (Para nuevos usuarios)

Si es tu primera vez con este proyecto, sigue estos pasos en orden:

1. **Crear ambiente virtual** (conda o venv) e instalar dependencias
2. **Configurar credenciales AWS** (crear archivo `.env` desde `.env.example`)
3. **Descargar datos desde S3** (ejecutar `dvc pull data/raw.dvc`)
4. **Iniciar servidor MLflow** (ejecutar `./start_mlflow.sh`)
5. **Ejecutar pipeline** (ejecutar `dvc repro --force`)

üìñ **Lee las secciones detalladas abajo si tienes dudas.**

‚è±Ô∏è **Tiempo total estimado:** 10-15 minutos (primera vez)

---

## SETUP INICIAL (Primera vez)

### 1. Crear y activar ambiente virtual

**Opci√≥n A: Con Conda (recomendado)**
```bash
# Crear ambiente
conda create -n proyectomlops python=3.11 -y

# Activar ambiente
conda activate proyectomlops

# Instalar dependencias
pip install -r requirements.txt
```

**Opci√≥n B: Con venv (Python nativo)**
```bash
# Crear ambiente virtual
python3 -m venv venv

# Activar ambiente
source venv/bin/activate  # En Mac/Linux
# O en Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

**Opci√≥n C: Si ya tienes un ambiente creado**
```bash
# Solo activar tu ambiente existente
conda activate proyectomlops  # Si usas conda
# O
source venv/bin/activate      # Si usas venv

# Instalar/actualizar dependencias
pip install -r requirements.txt
```

### 2. Configurar credenciales AWS
Las credenciales est√°n en el archivo `202502-equipo4_accessKeys.csv` (compartido por el equipo). 

```bash
# Copiar el template y editarlo con tus credenciales
cp .env.example .env

# Editar .env y reemplazar YOUR_ACCESS_KEY_ID y YOUR_SECRET_ACCESS_KEY
# con las credenciales del archivo accessKeys.csv
```

**Contenido del `.env`:**
```bash
# AWS S3 Credentials (obtener del archivo accessKeys.csv)
AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION=us-east-1

# MLflow Configuration
MLFLOW_TRACKING_URI=http://127.0.0.1:5000
MLFLOW_S3_BUCKET=itesm-mna
MLFLOW_ARTIFACT_ROOT=s3://itesm-mna/202502-equipo4/mlflow-artifacts
```

‚ö†Ô∏è **IMPORTANTE:** 
- Reemplazar `YOUR_ACCESS_KEY_ID` y `YOUR_SECRET_ACCESS_KEY` con las credenciales reales
- El archivo `.env` NO debe subirse a Git (ya est√° en `.gitignore`)

### 3. Descargar datos desde S3

Los datos est√°n versionados con DVC en S3. Desc√°rgalos con:

```bash
# Descargar datos raw desde S3
dvc pull data/raw.dvc

# Verificar que se descargaron correctamente
ls -lh data/raw/bike_sharing_modified.csv
# Debe mostrar: bike_sharing_modified.csv (1.6M)
```

**Si `dvc pull` falla con "Missing cache files":**

Esto significa que los datos no est√°n en S3 todav√≠a. Contacta al equipo para:
- Obtener el archivo original `bike_sharing_modified.csv`
- Colocarlo en `data/raw/bike_sharing_modified.csv`
- Luego alguien del equipo debe hacer:
  ```bash
  dvc add data/raw/
  dvc push data/raw.dvc
  git add data/raw.dvc
  git commit -m "chore: add raw data to DVC"
  git push
  ```

### 4. Iniciar servidor MLflow

**IMPORTANTE:** El servidor MLflow debe estar corriendo ANTES de ejecutar cualquier stage del pipeline.

```bash
# Dar permisos de ejecuci√≥n al script (solo primera vez)
chmod +x start_mlflow.sh

# Iniciar servidor en background
nohup ./start_mlflow.sh > mlflow_server.log 2>&1 &

# Esperar 5 segundos para que inicie
sleep 5

# Verificar que est√° corriendo (debe responder con HTML)
curl http://127.0.0.1:5000/
```

**Si ves el HTML", el servidor est√° listo. Si no:**
```bash
# Ver el log para detectar errores
tail -20 mlflow_server.log

# Posibles problemas:
# - Puerto 5000 ocupado: lsof -ti:5000 | xargs kill -9
# - Falta archivo .env: verificar que existe y tiene las credenciales
```

## EJECUTAR PIPELINE

### üéØ Gu√≠a R√°pida por Escenario

Elige el escenario que mejor describe tu situaci√≥n:

| Escenario | Comando | Tiempo |
|-----------|---------|--------|
| **Primera vez / Repo nuevo** | `dvc repro --force` | 5-7 min |
| **Descargar trabajo del equipo** | `dvc pull` ‚Üí `dvc repro` | 30-60 seg |
| **Desarrollo (cambios en c√≥digo)** | `dvc repro` | Variable |
| **Verificar que todo funciona** | `dvc repro` | < 1 seg |

---

### üìã Opci√≥n 1: Primera Vez o Repo Nuevo (Recomendado)

**Situaci√≥n:** Acabas de clonar el repo o quieres regenerar todo desde cero.

```bash
# 1. Activar ambiente y exportar credenciales
conda activate proyectomlops
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000

# 2. Ejecutar pipeline completo (FORZADO)
dvc repro --force
```

‚è±Ô∏è **Tiempo:** 5-7 minutos (incluye entrenamiento de modelos)

**¬øPor qu√© `--force`?**
- ‚úÖ Garantiza que todos los stages se ejecuten
- ‚úÖ No depende de archivos pre-existentes en S3
- ‚úÖ 100% reproducible en cualquier m√°quina

**Salida esperada:**
```
Running stage 'data'...
Running stage 'train'...        ‚Üê 2-3 minutos (GridSearchCV)
Running stage 'evaluate'...
Running stage 'visualize'...
Updating lock file 'dvc.lock'
```

---

### üì• Opci√≥n 2: Descargar Trabajo del Equipo (M√°s R√°pido)

**Situaci√≥n:** Alguien del equipo ya ejecut√≥ el pipeline y subi√≥ los resultados a S3.

```bash
# 1. Activar ambiente y exportar credenciales
conda activate proyectomlops
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000

# 2. Descargar TODO desde S3
dvc pull

# 3. Verificar (opcional)
dvc repro
```

‚è±Ô∏è **Tiempo:** 30-60 segundos (solo descargas)

**Salida esperada:**
```
# dvc pull:
14 files fetched

# dvc repro:
Stage 'data' didn't change, skipping
Stage 'train' didn't change, skipping
Stage 'evaluate' didn't change, skipping
Stage 'visualize' didn't change, skipping
Data and pipelines are up to date.
```

---

### üîß Opci√≥n 3: Desarrollo (Cambios en C√≥digo)

**Situaci√≥n:** Modificaste c√≥digo y quieres ver el impacto.

```bash
# Activar ambiente y exportar credenciales (como antes)
conda activate proyectomlops
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000

# Ejecutar pipeline (DVC detecta cambios autom√°ticamente)
dvc repro
```

**DVC detectar√° autom√°ticamente qu√© cambi√≥:**

#### Ejemplo 1: Cambios en `src/data.py`
```
Running stage 'data'...         ‚Üê Re-ejecuta
Running stage 'train'...        ‚Üê Re-ejecuta (depende de data)
Running stage 'evaluate'...     ‚Üê Re-ejecuta (depende de train)
Running stage 'visualize'...    ‚Üê Re-ejecuta (depende de evaluate)
```

#### Ejemplo 2: Cambios en `src/visualize.py`
```
Stage 'data' didn't change, skipping
Stage 'train' didn't change, skipping
Stage 'evaluate' didn't change, skipping
Running stage 'visualize'...    ‚Üê Solo re-ejecuta este
```

---

### üîÑ Despu√©s de Entrenar Modelos: Compartir con el Equipo

**Situaci√≥n:** Ejecutaste el pipeline y quieres compartir tus resultados.

```bash
# 1. Subir outputs a S3
dvc push

# 2. Verificar que se subi√≥ todo
dvc status -c
# Debe decir: "Cache and remote 'storage' are in sync."

# 3. Commitear cambios
git add dvc.lock models.dvc data/raw.dvc
git commit -m "chore: update pipeline outputs after training"
git push
```

**¬øQu√© se sube a S3?**
- ‚úÖ Modelos entrenados (`models/*.pkl`)
- ‚úÖ M√©tricas de evaluaci√≥n (`metrics/*.json`)
- ‚úÖ Reportes y gr√°ficas (`reports/*`)
- ‚úÖ Datos procesados (`data/processed/*`)

---

### ‚ö†Ô∏è Troubleshooting: Problemas Comunes con DVC

#### ‚ùå Error: "Stage didn't change, skipping" pero faltan archivos

**Causa:** DVC detecta que el c√≥digo no cambi√≥, pero no tienes los outputs localmente.

**Soluci√≥n:**
```bash
# Opci√≥n 1: Descargar desde S3
dvc pull

# Opci√≥n 2: Forzar re-ejecuci√≥n
dvc repro --force

# Opci√≥n 3: Limpiar y regenerar
rm -rf models/ metrics/ reports/ data/processed/
dvc repro --force
```

---

#### ‚ùå Error: "Missing cache files" o "failed to pull"

**Causa:** Los archivos no est√°n en S3 (nadie los subi√≥).

**Soluci√≥n:**
```bash
# Regenerar todo localmente
dvc repro --force

# Subir a S3 para el equipo
dvc push
```

---

#### ‚ùå Error: "Can't remove unsaved files without confirmation"

**Causa:** Tienes archivos locales que no est√°n en el cach√© de DVC.

**Soluci√≥n:**
```bash
# Forzar pull (sobrescribe archivos locales)
dvc pull --force

# O regenerar desde cero
rm -rf models/ metrics/ reports/ data/processed/
dvc repro --force
```

---

### üí° Comandos √ötiles de DVC

```bash
# Ver qu√© stages necesitan ejecutarse
dvc status

# Ver qu√© archivos necesitan subirse a S3
dvc status -c

# Descargar archivos espec√≠ficos desde S3
dvc pull data/raw.dvc        # Solo datos raw
dvc pull models.dvc          # Solo modelos
dvc pull                     # Todo lo trackeado

# Subir archivos espec√≠ficos a S3
dvc push data/raw.dvc        # Solo datos raw
dvc push models.dvc          # Solo modelos
dvc push                     # Todo lo trackeado

# Ver diferencias en m√©tricas entre runs
dvc metrics show

# Ver el DAG del pipeline
dvc dag

# Limpiar cach√© local no usado
dvc gc --workspace

# Forzar un stage espec√≠fico
dvc repro --force train
```

---

### üìö Flujo Completo de Trabajo en Equipo

#### üîÑ Ciclo de Desarrollo Colaborativo:

**Persona A (entrena modelos nuevos):**
```bash
# 1. Hacer cambios en c√≥digo
vim src/train_predict.py

# 2. Ejecutar pipeline
dvc repro

# 3. Subir resultados a S3
dvc push

# 4. Commitear y pushear
git add dvc.lock models.dvc
git commit -m "feat: improve model performance"
git push
```

**Persona B (usa los modelos de A):**
```bash
# 1. Obtener cambios
git pull

# 2. Descargar modelos desde S3
dvc pull

# 3. Verificar o continuar desarrollo
dvc repro
```

---

### üéØ Entendiendo el Caching de DVC

DVC usa **caching inteligente** para evitar trabajo innecesario:

#### Escenario 1: Sin cambios
```bash
$ dvc repro
Stage 'data' didn't change, skipping
Stage 'train' didn't change, skipping
Stage 'evaluate' didn't change, skipping
Stage 'visualize' didn't change, skipping
Data and pipelines are up to date.
```
‚è±Ô∏è Tiempo: < 1 segundo

#### Escenario 2: Cambios en c√≥digo de preprocesamiento
```bash
$ vim src/data.py  # Modificas limpieza de datos
$ dvc repro
Running stage 'data'...         ‚Üê Re-ejecuta (c√≥digo cambi√≥)
Running stage 'train'...        ‚Üê Re-ejecuta (datos cambiaron)
Running stage 'evaluate'...     ‚Üê Re-ejecuta (modelos cambiaron)
Running stage 'visualize'...    ‚Üê Re-ejecuta (m√©tricas cambiaron)
```
‚è±Ô∏è Tiempo: 5-7 minutos (todo el pipeline)

#### Escenario 3: Cambios solo en visualizaci√≥n
```bash
$ vim src/visualize.py  # Modificas gr√°ficas
$ dvc repro
Stage 'data' didn't change, skipping
Stage 'train' didn't change, skipping
Stage 'evaluate' didn't change, skipping
Running stage 'visualize'...    ‚Üê Solo re-ejecuta este
```
‚è±Ô∏è Tiempo: 10-15 segundos

#### Escenario 4: Descarga desde S3 (trabajo del equipo)
```bash
$ git pull
$ dvc pull
Stage 'train' is cached - checking out outputs    ‚Üê Descarga desde S3
Stage 'evaluate' is cached - checking out outputs ‚Üê Descarga desde S3
Stage 'visualize' is cached - checking out outputs ‚Üê Descarga desde S3
```
‚è±Ô∏è Tiempo: 30-60 segundos

---

### üî® Opci√≥n 4: Ejecutar Stages Individualmente (Avanzado)

Si prefieres ejecutar cada stage por separado para debugging o desarrollo:

### ‚ö†Ô∏è ANTES DE EJECUTAR CUALQUIER STAGE:

**1. Aseg√∫rate de tener el ambiente activado:**
```bash
conda activate proyectomlops  # Si usas conda
# O
source venv/bin/activate      # Si usas venv
```

**2. Aseg√∫rate de que el servidor MLflow est√© corriendo:**
```bash
curl http://127.0.0.1:5000/  # Debe responder con HTML
```

**3. Exportar credenciales AWS (reemplazar con tus credenciales del archivo accessKeys.csv):**
```bash
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000
```

**üí° TIP:** Puedes crear un script `set_env.sh` con estos exports para no escribirlos cada vez:
```bash
# Crear archivo set_env.sh
cat > set_env.sh << 'EOF'
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000
EOF

# Luego solo ejecutar:
source set_env.sh
```

### Ejecutar Stages en Orden

**Los stages deben ejecutarse en este orden:** DATA ‚Üí TRAIN ‚Üí EVALUATE ‚Üí VISUALIZE

#### Stage 1: DATA (Procesamiento de datos)
Procesa los datos crudos y genera conjuntos de train/test limpios.

```bash
python -m src.main \
    --stage=data \
    --csv data/raw/bike_sharing_modified.csv \
    --target cnt \
    --cleaned_train_csv data/processed/bike_sharing_train_cleaned.csv \
    --cleaned_test_csv data/processed/bike_sharing_test_cleaned.csv
```

**Salida esperada:**
- `data/processed/bike_sharing_train_cleaned.csv` (train set limpio)
- `data/processed/bike_sharing_test_cleaned.csv` (test set limpio)

---

#### Stage 2: TRAIN (Entrenamiento de modelos)
Entrena 3 modelos: Random Forest, Gradient Boosting y Ridge Regression.

```bash
python -m src.main \
    --stage=train \
    --cleaned_train_csv data/processed/bike_sharing_train_cleaned.csv \
    --target cnt \
    --models_dir models
```

**Salida esperada:**
- `models/random_forest.pkl`
- `models/gradient_boosting.pkl`
- `models/ridge_regression.pkl`
- Metadata JSON para cada modelo
- Artifacts en S3

‚è±Ô∏è **Tiempo estimado:** 2-5 minutos (GridSearchCV con 270 fits)

---

#### Stage 3: EVALUATE (Evaluaci√≥n de modelos)
Eval√∫a los modelos entrenados en el test set.

```bash
python -m src.main \
    --stage=evaluate \
    --models_dir models \
    --cleaned_test_csv data/processed/bike_sharing_test_cleaned.csv \
    --target cnt \
    --metrics_dir metrics
```

**Salida esperada:**
- `metrics/random_forest_test_results.json`
- `metrics/gradient_boosting_test_results.json`
- `metrics/ridge_regression_test_results.json`

---

#### Stage 4: VISUALIZE (Visualizaci√≥n y reportes)
Genera gr√°ficas de comparaci√≥n y reportes.

```bash
python -m src.main \
    --stage=visualize \
    --metrics_dir metrics \
    --reports_dir reports
```

**Salida esperada:**
- `reports/model_comparison.png` (gr√°fica de comparaci√≥n)
- `reports/model_comparison_results.csv` (tabla de resultados)
- `reports/performance_report.md` (reporte en Markdown)
- Artifacts en S3

## ACCEDER A MLFLOW UI
```bash
# Abrir en navegador
open http://127.0.0.1:5000
```

## DETENER SERVIDOR MLFLOW
```bash
pkill -f "mlflow server"
```

## TROUBLESHOOTING

### Problemas Comunes y Soluciones

#### ‚ùå Error: "ModuleNotFoundError: No module named 'pandas'"
**Causa:** No instalaste las dependencias o no activaste el ambiente.

**Soluci√≥n:**
```bash
# Activar ambiente
conda activate proyectomlops  # o source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

---

#### ‚ùå Error: "NoSuchBucket" o "Failed to upload to S3"
**Causa:** Credenciales AWS no configuradas o incorrectas.

**Soluci√≥n:**
```bash
# 1. Verificar que .env existe y tiene las credenciales correctas
cat .env

# 2. Exportar credenciales en la terminal
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1

# 3. Verificar acceso a S3
aws s3 ls s3://itesm-mna/202502-equipo4/
```

---

#### ‚ùå Error: "Address already in use" (puerto 5000)
**Causa:** Ya hay un proceso usando el puerto 5000.

**Soluci√≥n:**
```bash
# Matar proceso en puerto 5000
lsof -ti:5000 | xargs kill -9

# Reiniciar servidor MLflow
nohup ./start_mlflow.sh > mlflow_server.log 2>&1 &
sleep 5
curl http://127.0.0.1:5000/
```

---

#### ‚ùå Error: "FileNotFoundError: data/raw/bike_sharing_modified.csv"
**Causa:** Los datos no se descargaron desde S3.

**Soluci√≥n:**
```bash
# Descargar datos desde S3
dvc pull data/raw.dvc

# Verificar que se descargaron
ls -lh data/raw/bike_sharing_modified.csv
```

**Si `dvc pull` falla:**
- Contactar al equipo para obtener el archivo original
- Colocarlo manualmente en `data/raw/bike_sharing_modified.csv`

---

#### ‚ùå Error: "MLFLOW_TRACKING_URI not set"
**Causa:** Variable de entorno no exportada.

**Soluci√≥n:**
```bash
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000

# O verificar que el servidor MLflow est√© corriendo
curl http://127.0.0.1:5000/
```

---

#### ‚ùå El servidor MLflow no inicia
**Causa:** Error en el script o credenciales incorrectas.

**Soluci√≥n:**
```bash
# Ver el log de errores
tail -50 mlflow_server.log

# Verificar que .env existe
ls -la .env

# Intentar iniciar manualmente para ver errores
./start_mlflow.sh
```

---

#### üí° Verificar que todo est√° configurado correctamente

```bash
# 1. Ambiente activado
which python  # Debe mostrar ruta del ambiente virtual

# 2. Dependencias instaladas
pip list | grep -E "mlflow|pandas|scikit-learn|boto3"

# 3. Servidor MLflow corriendo
curl http://127.0.0.1:5000/  # Debe responder con HTML

# 4. Credenciales AWS configuradas
echo $AWS_ACCESS_KEY_ID  # Debe mostrar tu access key

# 5. Datos en lugar correcto
ls -lh data/raw/bike_sharing_modified.csv
```

---

### üìö M√°s Informaci√≥n

Ver `SETUP_INSTRUCTIONS.md` para una gu√≠a m√°s detallada.

Argumento	Requerido	Descripci√≥n
--stage	S√≠	Define la etapa del pipeline a ejecutar. Las opciones v√°lidas son:
data: procesa los datos y genera un CSV limpio.
train: entrena los modelos definidos en MODEL_CONFIGS.
evaluate: eval√∫a los modelos entrenados y genera m√©tricas.
visualize: genera gr√°ficas y reportes de desempe√±o.
--csv	S√≠ (solo para --stage=data)	Ruta al archivo CSV original que se procesar√°.
--processed_csv	No	Ruta del archivo CSV procesado (por defecto: data/processed/processed.csv). Usado por las etapas train y evaluate.
--models_dir	No	Directorio donde se guardan o cargan los modelos entrenados (por defecto: models/).
--metrics_dir	No	Directorio donde se guardan o leen las m√©tricas de evaluaci√≥n en formato JSON (por defecto: metrics/).
--reports_dir	No	Directorio donde se generan los gr√°ficos y reportes de desempe√±o (por defecto: reports/).
--target	No	Nombre de la variable objetivo (columna dependiente). Si no se especifica, se usa la √∫ltima columna del dataset o una llamada target.
--test_size	No	Proporci√≥n de datos destinados al conjunto de prueba. Valor por defecto: 0.2.
--random_state	No	Semilla aleatoria para asegurar reproducibilidad. Valor por defecto: 42.
